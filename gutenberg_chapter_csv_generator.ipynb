{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text related functionse to clean up raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_text_url(url: str) -> str:\n",
    "    '''\n",
    "    Takes a URL and returns the raw text from that URL.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        The URL to get the text from.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The raw text from the URL.\n",
    "    '''\n",
    "    r = requests.get(url)\n",
    "    return r.text\n",
    "\n",
    "def get_base_text_file(file_path: str) -> str:\n",
    "    '''\n",
    "    Reads the content of a text file and returns it as a string.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The path to the text file.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The content of the text file as a string.\n",
    "    '''\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "\n",
    "def clean_guttenberg_header(text: str) -> str:\n",
    "    '''\n",
    "    Cleans the header added by Project Gutenberg from the text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The raw text obtained from Project Gutenberg.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The text with the header removed.\n",
    "    '''\n",
    "    ex = r\"The Project Gutenberg eBook.*?\\*\\*\\* START OF THE PROJECT GUTENBERG EBOOK.*?\\*\\*\\*\"\n",
    "    main_text = re.sub(ex, \"\", text, flags=re.DOTALL)\n",
    "    return main_text\n",
    "\n",
    "def clean_guttenberg_footer(text: str) -> str:\n",
    "    '''\n",
    "    Cleans the footer added by Project Gutenberg from the text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text with the header cleaned.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The text with the footer removed.\n",
    "    '''\n",
    "    ex = r\"\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK.*\"\n",
    "    main_text = re.sub(ex, \"\", text, flags=re.DOTALL)\n",
    "    return main_text\n",
    "\n",
    "def clean_guttenberg(text: str) -> str:\n",
    "    '''\n",
    "    Cleans both the header and footer added by Project Gutenberg from the text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The raw text obtained from Project Gutenberg.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The cleaned text with both the header and footer removed.\n",
    "    '''\n",
    "    clean_text = clean_guttenberg_header(text)\n",
    "    clean_text = clean_guttenberg_footer(clean_text)\n",
    "    return clean_text\n",
    "\n",
    "def normalize_input(text: str) -> List[str]:\n",
    "    '''\n",
    "    Takes a string, removes stop words and punctuation, normalizes spaces and newlines,\n",
    "    converts text to lowercase, and returns the text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The input text to be normalized.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        A list of normalized words (tokens).\n",
    "    '''\n",
    "    normalized = text\n",
    "    normalized = re.sub(r'(\\.|\\?|\\!|,)\\s*', ' ', normalized)\n",
    "\n",
    "    # getting rid of multiple spaces\n",
    "    normalized = re.sub(r'[ \\t]+', ' ', normalized)\n",
    "    # getting rid of multiple new lines at a time\n",
    "    normalized = re.sub(r'\\n+', '\\n', normalized)\n",
    "    normalized = re.sub(r'\\s*\\n', '\\n', normalized)\n",
    "    \n",
    "    # making lowercase\n",
    "    normalized = normalized.lower()\n",
    "    # Can add other stuff like to clean up more as well if needed\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for spliting up text into chapters and saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to split up text into chapters\n",
    "def split_text_into_chapters(text: str, chapter_titles: List[str]) -> List[str]:\n",
    "    '''\n",
    "    Splits a text into chapters based on the given chapter titles.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text to be split into chapters.\n",
    "    chapter_titles : List[str]\n",
    "        A list of chapter titles used to find chapter breaks.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        A list of chapter contents.\n",
    "    '''\n",
    "    chapter_regex = \"|\".join(re.escape(title) for title in chapter_titles)\n",
    "    pattern = re.compile(chapter_regex, re.IGNORECASE)\n",
    "\n",
    "    chapter_positions = [m.start() for m in pattern.finditer(text)]\n",
    "\n",
    "    chapters = []\n",
    "\n",
    "    for i in range(len(chapter_positions)):\n",
    "        start = chapter_positions[i]\n",
    "        end = chapter_positions[i+1] if i+1 < len(chapter_positions) else None\n",
    "\n",
    "        chapter_context = text[start:end].strip() if end is not None else text[start:].strip()\n",
    "        chapters.append(chapter_context)\n",
    "\n",
    "    return chapters\n",
    "\n",
    "# Saving chapters to a CSV file\n",
    "def save_chapters_to_csv(chapters: List[str], file: str):\n",
    "    '''\n",
    "    Saves chapter contents to a CSV file with chapter numbers and content.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    chapters : List[str]\n",
    "        The list of chapter contents.\n",
    "    file : str\n",
    "        The path to the CSV file where the chapters will be saved.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    '''\n",
    "    df = pd.DataFrame({\n",
    "        'Chapter': [f'Chapter {i+1}' for i in range(len(chapters))],\n",
    "        'Content': chapters\n",
    "    })\n",
    "\n",
    "    df.to_csv(file, index=False, encoding='utf-8')\n",
    "\n",
    "    \n",
    "\n",
    "def process_file(file: str, chapter_titles: List[str], remove_table_of_contents: bool = False, table_of_contents: List[str] = None):\n",
    "    '''\n",
    "    Processes a single .txt file, optionally removes the table of contents, normalizes the content, \n",
    "    cleans it of Project Gutenberg headers and footers, splits it into chapters, and saves each as a .csv file \n",
    "    with the same name as the original .txt file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file : str\n",
    "        The path to the .txt file being processed.\n",
    "    chapter_titles : List[str]\n",
    "        A list of chapter titles used to split the text into chapters.\n",
    "    remove_table_of_contents : bool\n",
    "        If True, removes the first occurrence of each chapter title in the table of contents.\n",
    "    table_of_contents : List[str], optional\n",
    "        A separate list of titles for the table of contents. If not provided, it defaults to chapter titles.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    '''\n",
    "    csv_name = re.sub(r'\\.txt$', '.csv', file, flags=re.IGNORECASE)\n",
    "\n",
    "    text = get_base_text_file(file)\n",
    "\n",
    "    # Use chapter_titles as the default for table_of_contents if not specified\n",
    "    if table_of_contents is None:\n",
    "        table_of_contents = chapter_titles\n",
    "\n",
    "    # Optionally remove the first occurrence of each chapter title (Table of Contents)\n",
    "    if remove_table_of_contents:\n",
    "        for title in table_of_contents:\n",
    "            # Remove the first occurrence of each chapter title\n",
    "            text = re.sub(re.escape(title), '', text, count=1, flags=re.IGNORECASE)\n",
    "\n",
    "    # Normalize the text (you can customize this function as needed)\n",
    "    book_full = clean_guttenberg(text)\n",
    "    book_chapters = split_text_into_chapters(book_full, chapter_titles)\n",
    "    normalized_chapters = [normalize_input(chapter) for chapter in book_chapters]\n",
    "\n",
    "    # Save the chapters to a CSV file\n",
    "    save_chapters_to_csv(normalized_chapters, csv_name)\n",
    "\n",
    "    print(f\"Processed {file} and saved as {csv_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions used to generate chapter titles for chapter seperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_txt_files_in_directory(directory: str):\n",
    "    '''\n",
    "    Takes a directory path and prints the names of all .txt files in that directory.\n",
    "    Also prints the total number of .txt files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    directory : str\n",
    "        The path to the directory containing .txt files.\n",
    "    '''\n",
    "    txt_files = []\n",
    "\n",
    "    # Loop through all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Check if the file ends with .txt\n",
    "        if filename.endswith(\".txt\"):\n",
    "            txt_files.append(filename)\n",
    "            print(filename)\n",
    "    \n",
    "    # Print the total number of .txt files found\n",
    "    print(f\"Total number of .txt files: {len(txt_files)}\")\n",
    "\n",
    "def list_txt_path_in_directory(directory: str):\n",
    "    '''\n",
    "    Takes a directory path and prints the relative paths of all .txt files in the directory in the format:\n",
    "    book = \"relative/path/to/file.txt\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    directory : str\n",
    "        The path to the directory containing .txt files.\n",
    "    '''\n",
    "    txt_files = []\n",
    "\n",
    "    # Loop through all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Check if the file ends with .txt\n",
    "        if filename.endswith(\".txt\"):\n",
    "            # Get the relative path of the file\n",
    "            relative_path = os.path.join(directory, filename)\n",
    "            txt_files.append(relative_path)\n",
    "            # Print the relative path in the required format\n",
    "            print(f'book = \"{relative_path}\"')\n",
    "    \n",
    "    # Print the total number of .txt files found\n",
    "    print(f\"Total number of .txt files: {len(txt_files)}\")\n",
    "\n",
    "\n",
    "# Helper function to convert an integer to a Roman numeral\n",
    "def int_to_roman(n: int) -> str:\n",
    "    '''\n",
    "    Converts an integer to its Roman numeral representation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        The integer to be converted.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The Roman numeral representation of the integer.\n",
    "    '''\n",
    "    roman_numerals = {\n",
    "        1: 'i', 4: 'iv', 5: 'v', 9: 'ix', 10: 'x',\n",
    "        40: 'xl', 50: 'l', 90: 'xc', 100: 'c'\n",
    "    }\n",
    "    result = ''\n",
    "    \n",
    "    for value in sorted(roman_numerals.keys(), reverse=True):\n",
    "        while n >= value:\n",
    "            result += roman_numerals[value]\n",
    "            n -= value\n",
    "            \n",
    "    return result\n",
    "\n",
    "# Generate an array with chapter titles \"chapter i\", \"chapter ii\", ...\n",
    "def generate_roman_chapters(num_chapters: int) -> List[str]:\n",
    "    '''\n",
    "    Generates a list of chapter titles in the format \"chapter i\", \"chapter ii\", etc.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_chapters : int\n",
    "        The number of chapters to generate titles for.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        A list of chapter titles with Roman numerals.\n",
    "    '''\n",
    "    return [f\"chapter {int_to_roman(i)}\" for i in range(1, num_chapters + 1)]\n",
    "\n",
    "def text_to_array(text: str) -> List[str]:\n",
    "    '''\n",
    "    Takes a block of text and splits it into an array, with each element representing a line of text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The input text containing multiple lines.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        A list where each element is a line from the input text.\n",
    "    '''\n",
    "    # Split the text by newlines and strip any leading/trailing spaces\n",
    "    return [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "\n",
    "def remove_underscore_numbers(line):\n",
    "    # This regex looks for an underscore, one or more digits, and another underscore\n",
    "    return re.sub(r'_\\d+_', '', line)\n",
    "\n",
    "def remove_numbers(line):\n",
    "    # This regex looks for one or more digits in the line and removes them\n",
    "    return re.sub(r'\\d+', '', line)\n",
    "def format_chapter_titles_with_extra_line(chapters):\n",
    "    formatted_chapters = []\n",
    "    for chapter in chapters:\n",
    "        # Split the chapter title based on the first space\n",
    "        number, title = chapter.split(' ', 1)\n",
    "        # Format the chapter with an extra newline between the number and the title\n",
    "        formatted_chapter = number + '\\n\\n' + title\n",
    "        formatted_chapters.append(formatted_chapter)\n",
    "    return formatted_chapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each book can have its own format with regards to Chapter formats, I am manually adding parameters for each book to account for the numerous special cases. So if new books are added they will have to me manually adjusted. Though I will try to make helper functions for specific formats to make it easer to add new books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agitha Christi Books to CSV\n",
    "- Doing all 11 texts we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data/ac/poirot_investigates.txt and saved as data/ac/poirot_investigates.csv\n"
     ]
    }
   ],
   "source": [
    "## Agitha christi\n",
    "book = \"data/ac/poirot_investigates.txt\"\n",
    "chapter_titles = ['The Adventure of “The Western Star', 'The Tragedy at Marsdon Manor', 'The Adventure of the Cheap Flat', 'The Mystery of Hunter’s Lodge', 'The Million Dollar Bond Robbery', 'The Adventure of the Egyptian Tomb', 'Jewel Robbery at the _Grand Metropolitan_', 'The Kidnapped Prime Minister', 'The Disappearance of Mr. Davenheim', 'The Adventure of the Italian Nobleman', 'The Case of the Missing Will']\n",
    "process_file(book, chapter_titles, remove_table_of_contents=True)\n",
    "\n",
    "\n",
    "book = \"data/ac/the_big_four.txt\"\n",
    "chapter_titles = ['1. THE UNEXPECTED GUEST', '2. THE MAN FROM THE ASYLUM', '3. WE HEAR MORE ABOUT LI CHANG YEN', '4. THE IMPORTANCE OF A LEG OF MUTTON', '5. DISAPPEARANCE OF A SCIENTIST', '6. THE WOMAN ON THE STAIRS', '7. THE RADIUM THIEVES', '8. IN THE HOUSE OF THE ENEMY', '9. THE YELLOW JASMINE MYSTERY', '10. WE INVESTIGATE AT CROFTLANDS', '11. A CHESS PROBLEM', '12. THE BAITED TRAP', '13. THE MOUSE WALKS IN', '14. THE PEROXIDE BLONDE', '15. THE TERRIBLE CATASTROPHE', '16. THE DYING CHINAMAN', '17. NUMBER FOUR WINS A TRICK', '18. IN THE FELSENLABYRYNTH']\n",
    "process_file(book, chapter_titles, remove_table_of_contents=True)\n",
    "\n",
    "\n",
    "book = \"data/ac/the_hunters_lodge_case.txt\"\n",
    "chapter_titles = ['*       *       *       *       *', '*       *       *       *       *', '*       *       *       *       *', '*       *       *       *       *', '*       *       *       *       *', '*       *       *       *       *', '*       *       *       *       *', '*       *       *       *       *']\n",
    "process_file(book, chapter_titles, remove_table_of_contents=False)\n",
    "\n",
    "\n",
    "book = \"data/ac/the_man_in_the_brown_suit.txt\"\n",
    "chapter_titles = generate_roman_chapters(37)\n",
    "chapter_titles = [chapter.upper() for chapter in chapter_titles]\n",
    "chapter_titles.insert(0, 'PROLOGUE')\n",
    "process_file(book, chapter_titles, remove_table_of_contents=False)\n",
    "\n",
    "\n",
    "book = \"data/ac/the_missing_will.txt\"\n",
    "chapter_titles = ['*       *       *       *       *', '*       *       *       *       *', '*       *       *       *       *', '*       *       *       *       *', '*       *       *       *       *', '*       *       *       *       *']\n",
    "process_file(book, chapter_titles, remove_table_of_contents=False)\n",
    "\n",
    "\n",
    "book = \"data/ac/the_murder_of_roger_ackroyd.txt\"\n",
    "chapter_titles = generate_roman_chapters(27)\n",
    "chapter_titles = [chapter.upper() for chapter in chapter_titles]\n",
    "process_file(book, chapter_titles, remove_table_of_contents=False)\n",
    "\n",
    "\n",
    "book = \"data/ac/the_murder_on_the_links.txt\"\n",
    "chapter_titles = ['1 A Fellow Traveller', '2 An Appeal for Help', '3 At the Villa Geneviève', '4 The Letter Signed “Bella”', '5 Mrs. Renauld’s Story', '6 The Scene of the Crime', '7 The Mysterious Madame Daubreuil', '8 An Unexpected Meeting', '9 M. Giraud Finds Some Clues', '10 Gabriel Stonor', '11 Jack Renauld', '12 Poirot Elucidates Certain Points', '13 The Girl with the Anxious Eyes', '14 The Second Body', '15 A Photograph', '16 The Beroldy Case', '17 We Make Further Investigations', '18 Giraud Acts', '19 I Use My Grey Cells', '20 An Amazing Statement', '21 Hercule Poirot on the Case!', '22 I Find Love', '23 Difficulties Ahead', '24 “Save Him!”', '25 An Unexpected Dénouement', '26 I Receive a Letter', '27 Jack Renauld’s Story', '28 Journey’s End']\n",
    "process_file(book, chapter_titles, remove_table_of_contents=True)\n",
    "\n",
    "\n",
    "book = \"data/ac/the_mysterious_affair_at_styles.txt\"\n",
    "chapter_titles = generate_roman_chapters(13)\n",
    "chapter_titles = [chapter.upper() for chapter in chapter_titles]\n",
    "process_file(book, chapter_titles, remove_table_of_contents=True)\n",
    "\n",
    "\n",
    "book = \"data/ac/the_mystery_of_the_blue_train.txt\"\n",
    "chapter_titles = ['1. The Man with the White Hair', '2. M. le Marquis', '3. Heart of Fire', '4. In Curzon Street', '5. A Useful Gentleman', '6. Mirelle', '7. Letters', '8. Lady Tamplin Writes a Letter', '9. An Offer Refused', '10. On the Blue Train', '11. Murder', '12. At the Villa Marguerite', '13. Van Aldin Gets a Telegram', \"14. Ada Mason's Story\", '15. The Comte de la Roche', '16. Poirot Discusses the Case', '17. An Aristocratic Gentleman', '18. Derek Lunches', '19. An Unexpected Visitor', '20. Katherine Makes a Friend', '21. At the Tennis', '22. M. Papopolous Breakfasts', '23. A New Theory', '24. Poirot Gives Advice', '25. Defiance', '26. A Warning', '27. Interview with Mirelle', '28. Poirot Plays the Squirrel', '29. A Letter From Home', '30. Miss Viner Gives Judgment', '31. Mr. Aarons Lunches', '32. Katherine and Poirot Compare Notes', '33. A New Theory', '34. The Blue Train Again', '35. Explanations', '36. By the Sea']\n",
    "process_file(book, chapter_titles, remove_table_of_contents=True)\n",
    "\n",
    "\n",
    "book = \"data/ac/the_plymouth_express_affair.txt\"\n",
    "chapter_titles = ['*       *       *       *       *', '*       *       *       *       *', '*       *       *       *       *', '*       *       *       *       *', '*       *       *       *       *', '*       *       *       *       *', '*       *       *       *       *', '*       *       *       *       *', '*       *       *       *       *', '*       *       *       *       *']\n",
    "process_file(book, chapter_titles, remove_table_of_contents=False)\n",
    "\n",
    "\n",
    "book = \"data/ac/the_secret_adversary.txt\"\n",
    "chapter_titles = generate_roman_chapters(28)\n",
    "chapter_titles = [chapter.upper() for chapter in chapter_titles]\n",
    "chapter_titles.insert(0, 'PROLOGUE')\n",
    "process_file(book, chapter_titles, remove_table_of_contents=True)\n",
    "\n",
    "\n",
    "book = \"data/ac/the_secret_of_chimneys.txt\"\n",
    "toc = ['1 ANTHONY CADE SIGNS ON', '2 A LADY IN DISTRESS', '3 ANXIETY IN HIGH PLACES', '4 INTRODUCING A VERY CHARMING LADY', '5 FIRST NIGHT IN LONDON', '6 THE GENTLE ART OF BLACKMAIL', '7 MR. MCGRATH REFUSES AN INVITATION', '8 A DEAD MAN', '9 ANTHONY DISPOSES OF A BODY', '10 CHIMNEYS', '11 SUPERINTENDENT BATTLE ARRIVES', '12 ANTHONY TELLS HIS STORY', '13 THE AMERICAN VISITOR', '14 MAINLY POLITICAL AND FINANCIAL', '15 THE FRENCH STRANGER', '16 TEA IN THE SCHOOLROOM', '17 A MIDNIGHT ADVENTURE', '18 SECOND MIDNIGHT ADVENTURE', '19 SECRET HISTORY', '20 BATTLE AND ANTHONY CONFER', '21 MR. ISAACSTEIN’S SUIT-CASE', '22 THE RED SIGNAL', '23 ENCOUNTER IN THE ROSE GARDEN', '24 THE HOUSE AT DOVER', '25 TUESDAY NIGHT AT CHIMNEYS', '26 THE 13TH OF OCTOBER', '27 THE 13TH OF OCTOBER (_contd._)', '28 KING VICTOR', '29 FURTHER EXPLANATIONS', '30 ANTHONY SIGNS ON FOR A NEW JOB', '31 SUNDRY DETAILS']\n",
    "chapter_titles = ['1\\n\\nANTHONY CADE SIGNS ON', '2\\n\\nA LADY IN DISTRESS', '3\\n\\nANXIETY IN HIGH PLACES', '4\\n\\nINTRODUCING A VERY CHARMING LADY', '5\\n\\nFIRST NIGHT IN LONDON', '6\\n\\nTHE GENTLE ART OF BLACKMAIL', '7\\n\\nMR. MCGRATH REFUSES AN INVITATION', '8\\n\\nA DEAD MAN', '9\\n\\nANTHONY DISPOSES OF A BODY', '10\\n\\nCHIMNEYS', '11\\n\\nSUPERINTENDENT BATTLE ARRIVES', '12\\n\\nANTHONY TELLS HIS STORY', '13\\n\\nTHE AMERICAN VISITOR', '14\\n\\nMAINLY POLITICAL AND FINANCIAL', '15\\n\\nTHE FRENCH STRANGER', '16\\n\\nTEA IN THE SCHOOLROOM', '17\\n\\nA MIDNIGHT ADVENTURE', '18\\n\\nSECOND MIDNIGHT ADVENTURE', '19\\n\\nSECRET HISTORY', '20\\n\\nBATTLE AND ANTHONY CONFER', '21\\n\\nMR. ISAACSTEIN’S SUIT-CASE', '22\\n\\nTHE RED SIGNAL', '23\\n\\nENCOUNTER IN THE ROSE GARDEN', '24\\n\\nTHE HOUSE AT DOVER', '25\\n\\nTUESDAY NIGHT AT CHIMNEYS', '26\\n\\nTHE 13TH OF OCTOBER', '27\\n\\nThe 13th of October (contd.)', '28\\n\\nKING VICTOR', '29\\n\\nFURTHER EXPLANATIONS', '30\\n\\nANTHONY SIGNS ON FOR A NEW JOB', '31\\n\\nSUNDRY DETAILS']\n",
    "process_file(book, chapter_titles, remove_table_of_contents=True, table_of_contents=toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created new generic functions to try and speed up the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_generic(text: str, pattern: str) -> List[str]:\n",
    "    '''\n",
    "    Splits a text into sections based on the provided regex pattern.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text to be split.\n",
    "    pattern : str\n",
    "        The regex pattern used to find the section breaks.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        A list of section contents.\n",
    "    '''\n",
    "    compiled_pattern = re.compile(pattern, re.IGNORECASE)\n",
    "\n",
    "    # Find all positions where the pattern appears\n",
    "    section_positions = [m.start() for m in compiled_pattern.finditer(text)]\n",
    "\n",
    "    sections = []\n",
    "\n",
    "    # Split the text based on pattern positions\n",
    "    for i in range(len(section_positions)):\n",
    "        start = section_positions[i]\n",
    "        end = section_positions[i+1] if i+1 < len(section_positions) else None\n",
    "\n",
    "        # Extract the content for each section\n",
    "        section_content = text[start:end].strip() if end is not None else text[start:].strip()\n",
    "        sections.append(section_content)\n",
    "\n",
    "    return sections\n",
    "\n",
    "# Saving chapters to a CSV file\n",
    "def save_sections_to_csv(chapters: List[str], file: str):\n",
    "    '''\n",
    "    Saves chapter contents to a CSV file with chapter numbers and content.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    chapters : List[str]\n",
    "        The list of chapter contents (e.g., chapters).\n",
    "    file : str\n",
    "        The path to the CSV file where the chapters will be saved.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    '''\n",
    "    df = pd.DataFrame({\n",
    "        'Chapter': [f'Chapter {i+1}' for i in range(len(chapters))],\n",
    "        'Content': chapters\n",
    "    })\n",
    "\n",
    "    df.to_csv(file, index=False, encoding='utf-8')\n",
    "\n",
    "def process_file_generic(file: str, pattern: str, remove_contents: bool = False):\n",
    "    '''\n",
    "    Processes a single .txt file, optionally removes the first matching content based on the pattern, \n",
    "    splits it based on the provided pattern, and saves each chapter as a .csv file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file : str\n",
    "        The path to the .txt file being processed.\n",
    "    pattern : str\n",
    "        The regex pattern used to find the chapter breaks (e.g., \"CHAPTER [IVXLCDM]+\").\n",
    "    remove_contents : bool, optional\n",
    "        If True, removes the first matching portion of the text based on the same regex pattern.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    '''\n",
    "    # Generate the CSV file name\n",
    "    csv_name = re.sub(r'\\.txt$', '.csv', file, flags=re.IGNORECASE)\n",
    "\n",
    "    # Read the content of the .txt file\n",
    "    text = get_base_text_file(file)\n",
    "\n",
    "    # Normalize and clean the text (this is your custom function)\n",
    "    book_full = clean_guttenberg(text)\n",
    "\n",
    "    # Optionally remove the first occurrence of the matching pattern\n",
    "    if remove_contents:\n",
    "        # Remove the first matching chapter based on the pattern\n",
    "        book_full = re.sub(pattern, '', book_full, count=1, flags=re.IGNORECASE)\n",
    "\n",
    "    # Split the text using the single generic pattern\n",
    "    book_chapters = split_text_generic(book_full, pattern)\n",
    "\n",
    "    # Normalize each chapter\n",
    "    normalized_chapters = [normalize_input(chapter) for chapter in book_chapters]\n",
    "\n",
    "    # Save the chapters to a CSV file\n",
    "    save_sections_to_csv(normalized_chapters, csv_name)\n",
    "\n",
    "    print(f\"Processed {file} and saved as {csv_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Conan Doyle\n",
    "- Currently doing 10 books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data/cd/a_study_in_scarlet.txt and saved as data/cd/a_study_in_scarlet.csv\n",
      "Processed data/cd/adventures_of_sherlock_holmes.txt and saved as data/cd/adventures_of_sherlock_holmes.csv\n",
      "Processed data/cd/beyond_the_city.txt and saved as data/cd/beyond_the_city.csv\n",
      "Processed data/cd/danger!_and_other_stories.txt and saved as data/cd/danger!_and_other_stories.csv\n",
      "Processed data/cd/his_last_bow_an_epilogue_of_sherlock_holmes.txt and saved as data/cd/his_last_bow_an_epilogue_of_sherlock_holmes.csv\n",
      "Processed data/cd/micah_clarke.txt and saved as data/cd/micah_clarke.csv\n",
      "Processed data/cd/my_friend_the_murderer.txt and saved as data/cd/my_friend_the_murderer.csv\n",
      "Processed data/cd/songs_of_the_road.txt and saved as data/cd/songs_of_the_road.csv\n",
      "Processed data/cd/tales_of_terror_and_mystery.txt and saved as data/cd/tales_of_terror_and_mystery.csv\n",
      "Processed data/cd/the_adventure_of_wisteria_lodge.txt and saved as data/cd/the_adventure_of_wisteria_lodge.csv\n"
     ]
    }
   ],
   "source": [
    "# 14 total chapters\n",
    "book = \"data/cd/a_study_in_scarlet.txt\"\n",
    "pattern = r'CHAPTER [IVXLCDM]+\\.\\n'\n",
    "process_file_generic(book, pattern, remove_contents=False)\n",
    "\n",
    "# 12 total stories\n",
    "book = \"data/cd/adventures_of_sherlock_holmes.txt\"\n",
    "pattern = r'\\nAdventure [IVXLCDM]+\\n'\n",
    "process_file_generic(book, pattern, remove_contents=False)\n",
    "\n",
    "# 17 chapters\n",
    "book = \"data/cd/beyond_the_city.txt\"\n",
    "pattern = r'\\nCHAPTER [IVXLCDM]+\\. '\n",
    "process_file_generic(book, pattern, remove_contents=False)\n",
    "\n",
    "# 10 chapters, could maybe broken down more\n",
    "book = \"data/cd/danger!_and_other_stories.txt\"\n",
    "pattern = r'\\n[IVX]+\\. '\n",
    "process_file_generic(book, pattern, remove_contents=False)\n",
    "\n",
    "# I think this is a collection of 7 stories\n",
    "book = \"data/cd/his_last_bow_an_epilogue_of_sherlock_holmes.txt\"\n",
    "chapter_titles = [    'The Adventure of Wisteria Lodge\\n', 'The Adventure of the Bruce-Partington Plans\\n', 'The Adventure of the Devil’s Foot\\n', 'The Adventure of the Red Circle\\n', 'The Disappearance of Lady Frances Carfax\\n', 'The Adventure of the Dying Detective\\n', 'His Last Bow: The War Service of Sherlock Holmes\\n']\n",
    "process_file(book, chapter_titles, remove_table_of_contents=True)\n",
    "\n",
    "# 36 chapters\n",
    "book = \"data/cd/micah_clarke.txt\"\n",
    "pattern = r'\\nCHAPTER [IVX]+\\. '\n",
    "process_file_generic(book, pattern, remove_contents=False)\n",
    "\n",
    "# 2 parts \n",
    "book = \"data/cd/my_friend_the_murderer.txt\"\n",
    "pattern = r'(By A\\. Conan Doyle)|(\\*\\*\\*\\*\\*)'\n",
    "process_file_generic(book, pattern, remove_contents=False)\n",
    "\n",
    "# a bunch of songs\n",
    "book = \"data/cd/songs_of_the_road.txt\"\n",
    "chapter_titles = [\"A HYMN OF EMPIRE\", \"SIR NIGEL'S SONG\", \"THE ARAB STEED\", \"A POST-IMPRESSIONIST\", \"EMPIRE BUILDERS\", \"THE GROOM'S ENCORE\", \"THE BAY HORSE\", \"THE OUTCASTS\", \"THE END\", \"1902-1909\", \"THE WANDERER\", \"BENDY'S SERMON\", \"COMPENSATION\", \"THE BANNER OF PROGRESS\", \"HOPE\", \"RELIGIO MEDICI\", \"MAN'S LIMITATION\", \"MIND AND MATTER\", \"DARKNESS\", \"A WOMAN'S LOVE\", \"BY THE NORTH SEA\", \"DECEMBER'S SNOW\", \"SHAKESPEARE'S EXPOSTULATION\", \"THE EMPIRE\", \"A VOYAGE\", \"THE ORPHANAGE\", \"SEXAGENARIUS LOQUITUR\", \"NIGHT VOICES\", \"THE MESSAGE\", \"THE ECHO\", \"ADVICE TO A YOUNG AUTHOR\", \"A LILT OF THE ROAD\"]\n",
    "process_file(book, chapter_titles, remove_table_of_contents=True)\n",
    "\n",
    "# 2 6 chapter books\n",
    "book = \"data/cd/tales_of_terror_and_mystery.txt\"\n",
    "chapter_titles = [\"The Horror of the Heights\\n\", \"The Leather Funnel\\n\", \"The New Catacomb\\n\", \"The Case of Lady Sannox\\n\", \"The Terror of Blue John Gap\\n\", \"The Brazilian Cat\\n\", \"The Lost Special\\n\", \"The Beetle-Hunter\\n\", \"The Man with the Watches\\n\", \"The Japanned Box\\n\", \"The Black Doctor\\n\", \"The Jew's Breastplate\\n\"]\n",
    "process_file(book, chapter_titles, remove_table_of_contents=True)\n",
    "\n",
    "# No clear break in the book\n",
    "# book = \"data/cd/the_adventure_of_the_bruce-partington_plans.txt\"\n",
    "# chapter_titles = []\n",
    "# process_file(book, chapter_titles, remove_table_of_contents=True)\n",
    "\n",
    "# No clear break in the book\n",
    "# book = \"data/cd/the_adventure_of_the_cardboard_box.txt\"\n",
    "# chapter_titles = []\n",
    "# process_file(book, chapter_titles, remove_table_of_contents=True)\n",
    "# Same for data/cd/the_adventure_of_the_devils_foot.txt\n",
    "# and data/cd/the_adventure_of_the_dying_detective.txt\n",
    "\n",
    "book = \"data/cd/the_adventure_of_wisteria_lodge.txt\"\n",
    "chapter_titles = [\"1.  The Singular Experience of Mr. John Scott Eccles\", \"2.  The Tiger of San Pedro\"]\n",
    "process_file(book, chapter_titles, remove_table_of_contents=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G. K. Chesterton\n",
    "- Currently doing 10 books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data/gc/a_short_history_of_england.txt and saved as data/gc/a_short_history_of_england.csv\n",
      "Processed data/gc/all_things_considered.txt and saved as data/gc/all_things_considered.csv\n",
      "Processed data/gc/eugenics_and_other_evils.txt and saved as data/gc/eugenics_and_other_evils.csv\n",
      "Processed data/gc/heretics.txt and saved as data/gc/heretics.csv\n",
      "Processed data/gc/magic.txt and saved as data/gc/magic.csv\n",
      "Processed data/gc/manalive.txt and saved as data/gc/manalive.csv\n",
      "Processed data/gc/orthodoxy.txt and saved as data/gc/orthodoxy.csv\n",
      "Processed data/gc/robert_browning.txt and saved as data/gc/robert_browning.csv\n",
      "Processed data/gc/st._francis_of_assisi.txt and saved as data/gc/st._francis_of_assisi.csv\n",
      "Processed data/gc/the_ball_and_the_cross.txt and saved as data/gc/the_ball_and_the_cross.csv\n"
     ]
    }
   ],
   "source": [
    "# 18 chapters\n",
    "book = \"data/gc/a_short_history_of_england.txt\"\n",
    "pattern = r'\\n[IVX]+\\n{1,2}\\w+'\n",
    "process_file_generic(book, pattern, remove_contents=False)\n",
    "\n",
    "# 35 chaptesrs\n",
    "book = \"data/gc/all_things_considered.txt\"\n",
    "chapter_titles = titles = [\"\\nTHE CASE FOR THE EPHEMERAL\\n\", \"\\nCOCKNEYS AND THEIR JOKES\\n\", \"\\nTHE FALLACY OF SUCCESS\\n\", \"\\nON RUNNING AFTER ONE’S HAT\\n\", \"\\nTHE VOTE AND THE HOUSE\\n\", \"\\nCONCEIT AND CARICATURE\\n\", \"\\nPATRIOTISM AND SPORT\\n\", \"\\nAN ESSAY ON TWO CITIES\\n\", \"\\nFRENCH AND ENGLISH\\n\", \"\\nTHE ZOLA CONTROVERSY\\n\", \"\\nOXFORD FROM WITHOUT\\n\", \"\\nWOMAN\\n\", \"\\nTHE MODERN MARTYR\\n\", \"\\nON POLITICAL SECRECY\\n\", \"\\nEDWARD VII. AND SCOTLAND\\n\", \"\\nTHOUGHTS AROUND KOEPENICK\\n\", \"\\nTHE BOY\\n\", \"\\nLIMERICKS AND COUNSELS OF PERFECTION\\n\", \"\\nANONYMITY AND FURTHER COUNSELS\\n\", \"\\nON THE CRYPTIC AND THE ELLIPTIC\\n\", \"\\nTHE WORSHIP OF THE WEALTHY\\n\", \"\\nSCIENCE AND RELIGION\\n\", \"\\nTHE METHUSELAHITE\\n\", \"\\nSPIRITUALISM\\n\", \"\\nTHE ERROR OF IMPARTIALITY\\n\", \"\\nPHONETIC SPELLING\\n\", \"\\nHUMANITARIANISM AND STRENGTH\\n\", \"\\nWINE WHEN IT IS RED\\n\", \"\\nDEMAGOGUES AND MYSTAGOGUES\\n\", \"\\nTHE “EATANSWILL GAZETTE”\\n\", \"\\nFAIRY TALES\\n\", \"\\nTOM JONES AND MORALITY\\n\", \"\\nTHE MAID OF ORLEANS\\n\", \"\\nA DEAD POET\\n\", \"\\nCHRISTMAS\\n\"]\n",
    "process_file(book, chapter_titles, remove_table_of_contents=False)\n",
    "\n",
    "# 17 ch\n",
    "book = \"data/gc/eugenics_and_other_evils.txt\"\n",
    "chapter_titles = [\"\\nWHAT IS EUGENICS?\\n\", \"\\nTHE FIRST OBSTACLES\\n\", \"\\nTHE ANARCHY FROM ABOVE\\n\", \"\\nTHE LUNATIC AND THE LAW\\n\", \"\\nTHE FLYING AUTHORITY\\n\", \"\\nTHE UNANSWERED CHALLENGE\\n\", \"\\nTHE ESTABLISHED CHURCH OF DOUBT\\n\", \"\\nA SUMMARY OF A FALSE THEORY\\n\", \"\\nTHE IMPOTENCE OF IMPENITENCE\\n\", \"\\nTRUE HISTORY OF A TRAMP\\n\", \"\\nTRUE HISTORY OF A EUGENIST\\n\", \"\\nTHE VENGEANCE OF THE FLESH\\n\", \"\\nTHE MEANNESS OF THE MOTIVE\\n\", \"\\nTHE ECLIPSE OF LIBERTY\\n\", \"\\nTHE TRANSFORMATION OF SOCIALISM\\n\", \"\\nTHE END OF THE HOUSEHOLD GODS\\n\", \"\\nA SHORT CHAPTER\\n\"]\n",
    "process_file(book, chapter_titles, remove_table_of_contents=False)\n",
    "\n",
    "# 20 ch\n",
    "book = \"data/gc/heretics.txt\"\n",
    "pattern = r'\\n[IVX]+\\. '\n",
    "process_file_generic(book, pattern, remove_contents=False)\n",
    "\n",
    "# Prelude and 2 act starting at act 2, kinda hard to tell\n",
    "book = \"data/gc/magic.txt\"\n",
    "pattern = r'(THE PRELUDE|ACT [I]+)'\n",
    "process_file_generic(book, pattern, remove_contents=False)\n",
    "\n",
    "# 10 ch\n",
    "book = \"data/gc/manalive.txt\"\n",
    "chapter_titles = [\"How the Great Wind Came to Beacon House\", \"The Luggage of an Optimist\", \"The Banner of Beacon\", \"The Garden of the God\", \"The Allegorical Practical Joker\", \"The Eye of Death; or, the Murder Charge\", \"The Two Curates; or, the Burglary Charge\", \"The Round Road; or, the Desertion Charge\", \"The Wild Weddings; or, the Polygamy Charge\", \"How the Great Wind Went from Beacon House\"]\n",
    "process_file(book, chapter_titles, remove_table_of_contents=True)\n",
    "\n",
    "# 9 ch and had to add . for ch 4\n",
    "book = \"data/gc/orthodoxy.txt\"\n",
    "pattern = r'CHAPTER [IVX]+\\.-'\n",
    "process_file_generic(book, pattern, remove_contents=False)\n",
    "\n",
    "# 8 ch\n",
    "book = \"data/gc/robert_browning.txt\"\n",
    "pattern = r'CHAPTER [IVX]+\\n\\n'\n",
    "process_file_generic(book, pattern, remove_contents=False)\n",
    "\n",
    "# 10 ch\n",
    "book = \"data/gc/st._francis_of_assisi.txt\"\n",
    "pattern = r'_CHAPTER [IVX]+_'\n",
    "process_file_generic(book, pattern, remove_contents=False)\n",
    "\n",
    "# 20 ch\n",
    "book = \"data/gc/the_ball_and_the_cross.txt\"\n",
    "pattern = r'\\n\\n[IVX]+\\.'\n",
    "process_file_generic(book, pattern, remove_contents=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maurice Leblanc\n",
    "- Currently doing 10 books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data/ml/813.txt and saved as data/ml/813.csv\n",
      "Processed data/ml/arsène_lupin.txt and saved as data/ml/arsène_lupin.csv\n",
      "Processed data/ml/memoirs_of_arsène_lupin.txt and saved as data/ml/memoirs_of_arsène_lupin.csv\n",
      "Processed data/ml/the_blonde_lady.txt and saved as data/ml/the_blonde_lady.csv\n",
      "Processed data/ml/the_confessions_of_arsène_lupin.txt and saved as data/ml/the_confessions_of_arsène_lupin.csv\n",
      "Processed data/ml/the_crystal_stopper.txt and saved as data/ml/the_crystal_stopper.csv\n",
      "Processed data/ml/the_eight_strokes_of_the_clock.txt and saved as data/ml/the_eight_strokes_of_the_clock.csv\n",
      "Processed data/ml/the_eyes_of_innocence.txt and saved as data/ml/the_eyes_of_innocence.csv\n",
      "Processed data/ml/the_frontier.txt and saved as data/ml/the_frontier.csv\n",
      "Processed data/ml/the_golden_triangle_the_return_of_arsène_lupin.txt and saved as data/ml/the_golden_triangle_the_return_of_arsène_lupin.csv\n"
     ]
    }
   ],
   "source": [
    "# 17 ch including EPILOGUE\n",
    "book = \"data/ml/813.txt\"\n",
    "pattern = r'(CHAPTER [IVXLCDM]+|EPILOGUE)\\n\\n'\n",
    "process_file_generic(book, pattern, remove_contents=False)\n",
    "\n",
    "# 23\n",
    "book = \"data/ml/arsène_lupin.txt\"\n",
    "pattern = r'CHAPTER [IVX]+\\n'\n",
    "process_file_generic(book, pattern, remove_contents=False)\n",
    "\n",
    "# 15 including EPILOGUE\n",
    "book = \"data/ml/memoirs_of_arsène_lupin.txt\"\n",
    "pattern = r'(CHAPTER [IVX]+\\.)|(_EPILOGUE_)\\n'\n",
    "process_file_generic(book, pattern, remove_contents=False)\n",
    "\n",
    "# 8 ch\n",
    "book = \"data/ml/the_blonde_lady.txt\"\n",
    "pattern = r'CHAPTER [IVX]+\\n'\n",
    "process_file_generic(book, pattern, remove_contents=False)\n",
    "\n",
    "# 10 ch\n",
    "book = \"data/ml/the_confessions_of_arsène_lupin.txt\"\n",
    "pattern = r'\\n[IVX]+\\n'\n",
    "process_file_generic(book, pattern, remove_contents=False)\n",
    "\n",
    "# 13 ch\n",
    "book = \"data/ml/the_crystal_stopper.txt\"\n",
    "pattern = r'CHAPTER [IVX]+\\.\\n'\n",
    "process_file_generic(book, pattern, remove_contents=False)\n",
    "\n",
    "# 8 ch\n",
    "book = \"data/ml/the_eight_strokes_of_the_clock.txt\"\n",
    "pattern = r'\\n[IVX]+\\n'\n",
    "process_file_generic(book, pattern, remove_contents=False)\n",
    "\n",
    "# 11 ch\n",
    "book = \"data/ml/the_eyes_of_innocence.txt\"\n",
    "pattern = r'\\n[IVX]+\\n'\n",
    "process_file_generic(book, pattern, remove_contents=False)\n",
    "\n",
    "# 20 ch\n",
    "book = \"data/ml/the_frontier.txt\"\n",
    "pattern = r'CHAPTER [IVX]+\\n'\n",
    "process_file_generic(book, pattern, remove_contents=False)\n",
    "\n",
    "# 19 ch had to add CHAPTER to chapter 7\n",
    "book = \"data/ml/the_golden_triangle_the_return_of_arsène_lupin.txt\"\n",
    "pattern = r'CHAPTER [IVX]+\\n'\n",
    "process_file_generic(book, pattern, remove_contents=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakespeare\n",
    "- Currently doing 4 books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added altered functions to do scenes here quicker and automatically. Did before making the above generic function so that's why its not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_scenes(text: str) -> List[str]:\n",
    "    '''\n",
    "    Splits a text into scenes based on the pattern \"SCENE #\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text to be split into scenes.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        A list of scene contents.\n",
    "    '''\n",
    "    scene_regex = r'SCENE [IVXLCDM]+\\.'  # Matches \"SCENE I.\" or similar\n",
    "    pattern = re.compile(scene_regex, re.IGNORECASE)\n",
    "\n",
    "    # Find positions of all occurrences of \"SCENE #\"\n",
    "    scene_positions = [m.start() for m in pattern.finditer(text)]\n",
    "\n",
    "    scenes = []\n",
    "\n",
    "    # Split the text based on scene positions\n",
    "    for i in range(len(scene_positions)):\n",
    "        start = scene_positions[i]\n",
    "        end = scene_positions[i+1] if i+1 < len(scene_positions) else None\n",
    "\n",
    "        # Extract scene content\n",
    "        scene_content = text[start:end].strip() if end is not None else text[start:].strip()\n",
    "        scenes.append(scene_content)\n",
    "\n",
    "    return scenes\n",
    "\n",
    "\n",
    "def process_file_scenes(file: str):\n",
    "    '''\n",
    "    Processes a single .txt file, normalizes the content, \n",
    "    cleans it of Project Gutenberg headers and footers, splits it into scenes, and saves each as a .csv file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file : str\n",
    "        The path to the .txt file being processed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    '''\n",
    "    # Generate the CSV file name\n",
    "    csv_name = re.sub(r'\\.txt$', '.csv', file, flags=re.IGNORECASE)\n",
    "\n",
    "    # Read the content of the .txt file\n",
    "    text = get_base_text_file(file)\n",
    "\n",
    "    # Normalize and clean the text (this is your custom function)\n",
    "    book_full = clean_guttenberg(text)\n",
    "\n",
    "    # Split the text into scenes\n",
    "    book_sections = split_text_into_scenes(book_full)\n",
    "\n",
    "    # Normalize each scene (if you need to clean each section individually)\n",
    "    normalized_sections = [normalize_input(section) for section in book_sections]\n",
    "\n",
    "    # Save the scenes to a CSV file\n",
    "    save_chapters_to_csv(normalized_sections, csv_name)\n",
    "\n",
    "    print(f\"Processed {file} and saved as {csv_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data/shakespeare/coriolanus.txt and saved as data/shakespeare/coriolanus.csv\n",
      "Processed data/shakespeare/hamlet.txt and saved as data/shakespeare/hamlet.csv\n",
      "Processed data/shakespeare/macbeth.txt and saved as data/shakespeare/macbeth.csv\n",
      "Processed data/shakespeare/othello.txt and saved as data/shakespeare/othello.csv\n"
     ]
    }
   ],
   "source": [
    "book = \"data/shakespeare/coriolanus.txt\"\n",
    "process_file_scenes(book)\n",
    "\n",
    "book = \"data/shakespeare/hamlet.txt\"\n",
    "process_file_scenes(book)\n",
    "\n",
    "book = \"data/shakespeare/macbeth.txt\"\n",
    "process_file_scenes(book)\n",
    "\n",
    "book = \"data/shakespeare/othello.txt\"\n",
    "process_file_scenes(book)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
